{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/20KMJ/ESAA7/blob/main/%EB%AA%A8%EB%8D%B8%ED%9B%88%EB%A0%A8_%EC%97%B0%EC%8A%B5%EB%AC%B8%EC%A0%9C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **| 모델 훈련 연습 문제**\n",
        "___\n",
        "- 출처 : 핸즈온 머신러닝 Ch04 연습문제 1, 5, 9, 10\n",
        "- 개념 문제의 경우 텍스트 셀을 추가하여 정답을 적어주세요."
      ],
      "metadata": {
        "id": "zCu72vDHGMHo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. 수백만 개의 특성을 가진 훈련 세트에서는 어떤 선형 회귀 알고리즘을 사용할 수 있을까요?**\n",
        "___\n"
      ],
      "metadata": {
        "id": "j3g-_Dq9GiuT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "경사 하강법 (배치 경사 하강법, 확률적 경사 하강법, 미니배치 경사 하강법)"
      ],
      "metadata": {
        "id": "Mj-525Wf5bxY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2. 배치 경사 하강법을 사용하고 에포크마다 검증 오차를 그래프로 나타내봤습니다. 검증 오차가 일정하게 상승되고 있다면 어떤 일이 일어나고 있는 걸까요? 이 문제를 어떻게 해결할 수 있나요?**\n",
        "___"
      ],
      "metadata": {
        "id": "-pDjW5XcHPOt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "학습률이 너무 크기 때문에 이리저리 널뛰면서 최적점에서 점점 멀어져 발산하고 있을 수 있다. 그리드 탐색을 통해해 적절한 학습률을 찾아야 한다."
      ],
      "metadata": {
        "id": "8yRgJWVi7Egz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3. 릿지 회귀를 사용했을 때 훈련 오차가 검증 오차가 거의 비슷하고 둘 다 높았습니다. 이 모델에는 높은 편향이 문제인가요, 아니면 높은 분산이 문제인가요? 규제 하이퍼파라미터 $\\alpha$를 증가시켜야 할까요 아니면 줄여야 할까요?**\n",
        "___"
      ],
      "metadata": {
        "id": "nM7JbsLoy7b7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 해당 경우는 **높은 편향**값으로 인해 알고리즘이 데이터의 특징과 결과물과의 적절한 관계를 놓치게 만드는 과소적합 문제가 발생한 것으로 보인다. 따라서 규제 하이퍼파라미터 α를 **줄여** 모델의 제약을 줄여줘야한다."
      ],
      "metadata": {
        "id": "nPdaqTCT8QvW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4. 다음과 같이 사용해야 하는 이유는?**\n",
        "___\n",
        "- 평범한 선형 회귀(즉, 아무런 규제가 없는 모델) 대신 릿지 회귀\n",
        "- 릿지 회귀 대신 라쏘 회귀\n",
        "- 라쏘 회귀 대신 엘라스틱넷"
      ],
      "metadata": {
        "id": "C8tARu-ZzOGx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. 적어도 규제가 약간 있는 것이 대부분의 경우에 좋으므로 일반적인 평범한 선형 회귀는 피해야 한다.\n",
        "2. 릿지 회귀가 기본이 되지만 쓰이는 특성이 몇 개뿐이라고 의심되면 라쏘 회귀가 더 낫다.\n",
        "3. 특성 수가 훈련 샘플 수보다 많거나 특성 몇 개가 강하게 연관되어 있을 때는 보통 라쏘 회귀가 문제를 일으키므로 엘라스틱넷이 더 낫다."
      ],
      "metadata": {
        "id": "2lJEFccx9ii6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5. 조기 종료를 사용한 배치 경사 하강법으로 소프트맥스 회귀를 구현해보세요(사이킷런은 사용하지 마세요)**\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "QIZpOEYJVIAV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "from sklearn import datasets\n",
        "iris = datasets.load_iris()\n",
        "X = iris['data'][:,(2,3)]  # 꽃잎 길이, 꽃잎 넓이\n",
        "y = iris['target']\n",
        "\n",
        "\n",
        "# Add Bias\n",
        "X_with_bias = np.c_[np.ones([len(X),1]),X]   \n",
        "# len(X) 개수 만큼 1로 채워진 [ ], [ ]......[ ] array\n",
        "# 2열 -> 3열로 늘어남\n",
        "\n",
        "\n",
        "# 결과를 일정하게 하기 위해, random seed 배정\n",
        "np.random.seed(1234)\n",
        "\n",
        "\n",
        "# train/test split\n",
        "test_ratio = 0.2\n",
        "validation_ratio = 0.2\n",
        "total_size = len(X_with_bias)\n",
        "\n",
        "test_size = int(total_size * test_ratio)\n",
        "validation_size = int(total_size * validation_ratio)\n",
        "train_size = total_size - test_size - validation_size\n",
        "\n",
        "rnd_indices = np.random.permutation(total_size)    # 150을 무작위로 섞음 \n",
        "\n",
        "X_train = X_with_bias[rnd_indices[:train_size]]\n",
        "y_train = y[rnd_indices[:train_size]]\n",
        "X_valid = X_with_bias[rnd_indices[train_size:-test_size]]  # test_size 만큼 남겨둠\n",
        "y_valid = y[rnd_indices[train_size:-test_size]]\n",
        "X_test = X_with_bias[rnd_indices[-test_size:]]   \n",
        "y_test = y[rnd_indices[-test_size:]]\n",
        "\n",
        "\n",
        "# 클래스를 OneHot Vector로 바꾸기 \n",
        "def to_one_hot(y):\n",
        "    n_classes = y.max()+1    # 0,1,2 라 max=2 / +1 하면 classes 개수\n",
        "    m = len(y)                     # 총 150개의 라벨들\n",
        "    y_one_hot = np.zeros((m,n_classes))\n",
        "    y_one_hot[np.arange(m),y] = 1   # index의 행중에 y값을 1로 치환\n",
        "    return y_one_hot\n",
        "\n",
        "\n",
        "# 라벨 전부를 onehot encoding하기\n",
        "y_train_one_hot = to_one_hot(y_train)\n",
        "y_valid_one_hot = to_one_hot(y_valid)\n",
        "y_test_one_hot = to_one_hot(y_test)\n",
        "\n",
        "\n",
        "# Softmax 함수 만들기\n",
        "def softmax(logits):\n",
        "    exps = np.exp(logits)\n",
        "    exp_sums = np.sum(exps,axis=1,keepdims=True)   # axis=1   ->  가장 안쪽의 [ ] 안의 성분의 합 / 각 exps들의 합\n",
        "    return exps/exp_sums\n",
        "\n",
        "\n",
        "# 입력과 출력의 개수 정하기\n",
        "n_inputs = X_train.shape[1]   # (90,3) 인데 1로 인덱싱 ==3\n",
        "n_outputs = len(np.unique(y_train))  # y_train값을 중복되지 않는 값들을 출력  3\n",
        "\n",
        "\n",
        "eta = 0.01\n",
        "n_iteration = 5001\n",
        "m = len(X_train)\n",
        "epsilon = 1e-7 # ε : 입실론     nan값을 피하기 위해 logPi에 추가.\n",
        "\n",
        "Theta = np.random.randn(n_inputs,n_outputs)\n",
        "\n",
        "for i in range(n_iteration):\n",
        "    logits = X_train.dot(Theta)\n",
        "    Y_proba = softmax(logits)\n",
        "    loss = -np.mean(np.sum(y_train_one_hot * np.log(Y_proba + epsilon), axis=1))\n",
        "    error = Y_proba - y_train_one_hot\n",
        "    if i % 500 == 0:\n",
        "        print(i,loss)\n",
        "    gradients = 1/m * X_train.T.dot(error)\n",
        "    Theta = Theta - eta * gradients \n",
        "\n",
        "\n",
        "# 검증 세트에 대한 정확도 확인 \n",
        "logits = X_valid.dot(Theta)\n",
        "y_proba = softmax(logits)\n",
        "y_predict = np.argmax(y_proba, axis=1)\n",
        "\n",
        "accuracy = np.mean(y_predict == y_valid)\n",
        "accuracy"
      ],
      "metadata": {
        "id": "YoqWExgJAl-Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}